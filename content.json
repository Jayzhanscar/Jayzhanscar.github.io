{"meta":{"title":"詹灵杰博客","subtitle":null,"description":"当你真心渴望追求某种事物的话，整个宇宙都会联合起来帮你完成。  ——《牧羊少年奇幻之旅》","author":"zhanlingjie","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2019-02-03T17:43:15.000Z","updated":"2019-02-03T19:18:31.488Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"姓名：詹灵杰 职业： python研发工程师 联系方式: 18895309883 / 734422495 / zlj941020 简介： 目前居住在杭州梦想小镇，喜欢听民谣."},{"title":"categories","date":"2019-02-03T18:50:50.000Z","updated":"2019-02-03T18:50:50.899Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-02-03T18:09:37.000Z","updated":"2019-02-03T18:09:37.281Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"实用的文档","slug":"实用的文档","date":"2019-04-12T08:21:32.000Z","updated":"2019-04-12T08:37:34.823Z","comments":true,"path":"2019/04/12/实用的文档/","link":"","permalink":"http://yoursite.com/2019/04/12/实用的文档/","excerpt":"知识方案性文档","text":"知识方案性文档 实用的博客推荐 python 官方文档 socket详解 utiltest 详解 Http 协议详解 100亿条数据处理过程 线程互斥所以及信号量的介绍 django的信号量介绍 python web学习路线 python 对于传值类型的介绍 关于python垃圾回收机制 Tcp k8s django 部署推荐 mysql 索引数据结构以及算法实现 解决跨域请求 vuex 正则表达式 get 和 post的区别 rustful风格 docker 容器文档 docker portainer python I/o多路复用 mysql redis双写一致方案","categories":[],"tags":[{"name":"doc","slug":"doc","permalink":"http://yoursite.com/tags/doc/"}]},{"title":"python redis 操作","slug":"python-redis-操作","date":"2019-03-11T02:32:37.000Z","updated":"2019-03-11T02:48:49.828Z","comments":true,"path":"2019/03/11/python-redis-操作/","link":"","permalink":"http://yoursite.com/2019/03/11/python-redis-操作/","excerpt":"python 关于redis的常见操作","text":"python 关于redis的常见操作 12345678910111213141516171819202122232425262728293031323334pip install redisimport redis# 创建连接con1 = redis.Redis(host='127.0.0.1', port=6379)# 插入数据# set(name, value, ex=None, px=None, nx=False, xx=False)# 参数 ex，过期时间（秒） px，过期时间（毫秒） nx，如果设置为True，则只有name不存在时，当前set操作才执行 xx，如果设置为True，则只有name存在时，岗前set操作才执行# 批量插入 mset(*args, **kwargs)# 批量获取 mget(keys, *args)# 设置新值并获取原来的值getset(name, value)# 获取子序列， 比如v的特定长度字符getrange(key, start, end)# 设置子序列， 比如v的特定长度字符getrange(key, start, end)# 对key对应值的二进制setbit(name, offset, value)# 对key的值对应的二进制中某位的值getbit(name, offset)","categories":[],"tags":[{"name":"reids","slug":"reids","permalink":"http://yoursite.com/tags/reids/"}]},{"title":"python语言特性","slug":"python语言特性","date":"2019-02-12T02:02:16.000Z","updated":"2019-02-12T06:31:07.121Z","comments":true,"path":"2019/02/12/python语言特性/","link":"","permalink":"http://yoursite.com/2019/02/12/python语言特性/","excerpt":"工作久了对之前的基础语法做一个整理，以便自己更了解python的语法， 在编码的时候可以更加更加得心应手。","text":"工作久了对之前的基础语法做一个整理，以便自己更了解python的语法， 在编码的时候可以更加更加得心应手。 1. Python函数参数传递 先看两个例子 12345678910111213In [20]: a = 1In [21]: id(a)Out[21]: 4482033600In [22]: def func(a): ...: a = 2 ...: print(id(a), a) ...:In [23]: func(a)4482033632 2In [24]: 所有的变量都可以理解是内存中一个对象的“引用”， 通过id查看变量内存地址 在命名变量a查看的id和在函数中变量a的id是不一样的， 所以作用域的不同会生成两个不同的内存地址1234567891011In [24]: b = 1In [25]: def fun(b): ...: b = 2 ...: print(id(b), b) ...:In [26]: fun(b)4482033632 2In [27]: 在定义b变量之后查看id 和上一个a是类似的关系， 但是当查看全局变量b的时候发现id和第一个例子中函数中a变量的内存地址相同， 所以在python中当值类型相同时， 指针会指向同一个内存地址以减少内存的使用。 2. Python变量 在Python中有常见的string, int, list，float, tuple, dic 其中不可变对象: string, int , float, tuple, 可变对象: list, dictionary 在python中变量存放的是对象引用，虽然对象引用是可变的， 但是对象本身是不变的， 指针不会指向新的内存空间. 3. @staticmethod 和 @classmethod 在python中方法的类型有静态方法, 类方法和实例方法 代码如下: 123456789101112131415161718192021222324252627282930313233343536373839def foo(x): print(x)class A(object): \"\"\" 创建类 \"\"\" def foo(self, x): # 实例方法 ... @classmethod def class_foo(cls, x) # 类方法 ... @staticmethod def static_foo(x): # 静态方法 ...a = A()a.foo(1)A.class_foo(1)A.static(1)a.class_foo(1)a.static(1)``` 创建A类的时候实例方法必须是要创建一个实例a， 并且通过实例a调用实例方法， 但是不能直接用过A类直接调用， 但是实例a通过也可以调用A类的静态方法和类方法。### 4. 类变量和实例变量---类变量: 在创建类的时候定义，并且是所有实例之间共享的值.实例变量: 实例化之后， 每个实例单独拥有的变量，在创建实例中传值 In [40]: class A(object): …: num = 0 …: def init(self, name): …: self.name = name …: A.num += 1 …: print(name, A.num) …: …: …: In [41]: a = A(‘jay’)jay 1 In [42]: b = A(‘jack’)jack 2 In [43]: A.numOut[43]: 2 In [44]: 1234567891011121314### 5.Python自省模式---在定义变量的时候不用声明变量类型， 在程序运行时需要动态获取对象类型,type(),dir(),getattr(),hasattr(),isinstance().### 6.字典推导式和列表推导式---```pythonv = &#123;key: value for (key, value) in iterable &#125;l = [x for x in list] 7.单下划线和双下划线 12345678910In [45]: class B(object): ...: def __init__(self): ...: self.__superprivate = 'hello' ...: self._semprivate = 'word' ...:In [46]: m = B()In [49]: m.__dict__Out[49]: &#123;'_B__superprivate': 'hello', '_semprivate': 'word'&#125; foo类型: python内部函数的命名方式， 用来区别其他用户自定义的命名冲突,常见的init，new等等, _foo: 类的私有变量命名方式，在该类被实例化或者被引用的时候访问不到的变量 foo: 解析器会用_classnamefoo来替代这个名字，以区别和其他类重复命名的冲突，通过对象名._类名__xxx这样的方式可以访问. 8. 迭代器和生成器 在创建列表时，由于受到内存限制， 列表容量有限， 当一次性生成大量数据的列表时会造成大量的内存浪费， 所以在python中采用生成器:边循环边计算的机制 -&gt; generator 1234567&gt;&gt;&gt; L = [x*x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x*x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x0000028F8B774200&gt; 9. * args和**kwargs 用*args和**kwargs只是为了方便并没有强制使用它们. 当你不确定你的函数里将要传递多少参数时你可以用*args.例如,它可以传递任意数量的参数: 1234567891011121314151617181920&gt;&gt;&gt; def print_everything(*args): for count, thing in enumerate(args):... print &apos;&#123;0&#125;. &#123;1&#125;&apos;.format(count, thing)...&gt;&gt;&gt; print_everything(&apos;apple&apos;, &apos;banana&apos;, &apos;cabbage&apos;)0. apple1. banana2. cabbage相似的,**kwargs允许你使用没有事先定义的参数名:&gt;&gt;&gt; def table_things(**kwargs):... for name, value in kwargs.items():... print &apos;&#123;0&#125; = &#123;1&#125;&apos;.format(name, value)...&gt;&gt;&gt; table_things(apple = &apos;fruit&apos;, cabbage = &apos;vegetable&apos;)cabbage = vegetableapple = fruit你也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给*args和**kwargs.命名参数在列表的最前端.例如:def table_things(titlestring, **kwargs) args和**kwargs可以同时在函数的定义中,但是args必须在**kwargs前面. 当调用函数时你也可以用*和**语法.例如: 123456&gt;&gt;&gt; def print_three_things(a, b, c):... print 'a = &#123;0&#125;, b = &#123;1&#125;, c = &#123;2&#125;'.format(a,b,c)...&gt;&gt;&gt; mylist = ['aardvark', 'baboon', 'cat']&gt;&gt;&gt; print_three_things(*mylist)a = aardvark, b = baboon, c = cat 就像你看到的一样,它可以传递列表(或者元组)的每一项并把它们解包.注意必须与它们在函数里的参数相吻合.当然,你也可以在函数定义或者函数调用时用*. 10. 面向切面编程AOP和装饰器 装饰器是一个非常有用的设计， 经常被用于有切面需求的场景, 较为经典的的插入日志，性能测试、事务处理等。可以抽离出大量函数中与函数本身无关的雷同代码病继续重用， 其实就是给函数添加额外功能。 11. 鸭子类型 “当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。” 我们并不关心对象是什么类型，到底是不是鸭子，只关心行为。 比如在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 又比如list.extend()方法中,我们并不关心它的参数是不是list,只要它是可迭代的,所以它的参数可以是list/tuple/dict/字符串/生成器等. 鸭子类型在动态语言中经常使用，非常灵活，使得python不想java那样专门去弄一大堆的设计模式。 12.Python中的重载 之所以要重载， 主要是为了改变可变参数类型或者可变参数个数，面向对象思想了解的情况下很容易理解重载。 13. 新式类和旧式类 这篇文章很好的介绍了新式类的特性: http://www.cnblogs.com/btchenguang/archive/2012/09/17/2689146.html 新式类很早在2.2就出现了,所以旧式类完全是兼容的问题,Python3里的类全部都是新式类.这里有一个MRO问题可以了解下(新式类继承是根据C3算法,旧式类是深度优先),&lt;Python核心编程&gt;里讲的也很多. 一个旧式类的深度优先的例子 12345678910111213141516class A(): def foo1(self): print \"A\"class B(A): def foo2(self): passclass C(A): def foo1(self): print \"C\"class D(B, C): passd = D()d.foo1()# A 按照经典类的查找顺序从左到右深度优先的规则，在访问d.foo1()的时候,D这个类是没有的..那么往上查找,先找到B,里面没有,深度优先,访问A,找到了foo1(),所以这时候调用的是A的foo1()，从而导致C重写的foo1()被绕过 14. new和init的区别 new是一个静态方法,而init是一个实例方法.new方法会返回一个创建的实例,而init什么都不返回.只有在new返回一个cls的实例时后面的init才能被调用.当创建一个新实例时调用new,初始化一个实例时用init. ps: metaclass是创建类时起作用.所以我们可以分别使用metaclass,new和init来分别在类创建,实例创建和实例初始化的时候做一些小手脚. 15. 单例模式 单例模式是一种软件常用的设计模式， 在它核心结构中只包含一个被称为单例类的特殊类。通过单例模式可以保证一个类中只有一个实例并且这个实例易于被外界访问， 从而对实例个数的控制并且节约系统内存资源new()在init()之前被调用，用于生成实例对象。利用这个方法和类的属性的特点可以实现设计模式的单例模式。单例模式是指创建唯一对象，单例模式设计的类只能实例 这个绝对常考啊.绝对要记住1~2个方法,当时面试官是让手写的. 单例模式的实现方法: 使用new方法 1234567891011class Singleton(object): def __new__(cls, *args, **kwargs): if not hasttr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls, *args, **kwargs) return cls._instanceclass MyClass(Singleton): a = 1 共享模式创建实例时把所有实例的dict指向同一个字典， 这样它们具有相同的属性和方法。 12345678910class Borg(object): _state = &#123;&#125; def __new__(cls, *args, **kw): ob = super(Borg, cls).__new__(cls, *args, **kw) ob.__dict__ = cls._state return obclass MyClass2(Borg): a = 1 装饰器版本 1234567891011def singleton(cls): instances = &#123;&#125; def getinstance(*args, **kw): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance@singletonclass MyClass: ... import方法 1234567891011# mysingleton.pyclass My_Singleton(object): def foo(self): passmy_singleton = My_Singleton()# to usefrom mysingleton import my_singletonmy_singleton.foo() Python中的作用域 Python中， 一个变量的作用域总是在代码中被赋值的地方所决定的。 当Python遇到一个变量的话他会按照这样的顺序进行搜索 本地作用域(local) -&gt; 当前作用域被嵌入的本地作用域( Enclosing locals) -&gt; 全局/模块作用域(built-in) 18 GIL线程全局锁 线程全局锁(Global Interpreter Lock),即Python为了保证线程安全而采取的独立线程运行的限制,说白了就是一个核只能在同一时间运行一个线程.对于io密集型任务，python的多线程起到作用，但对于cpu密集型任务，python的多线程几乎占不到任何优势，还有可能因为争夺资源而变慢。 解决办法就是多进程和下面的协程(协程也只是单CPU,但是能减小切换代价提升性能). 19. 协程 简单点说协程是进程和线程的升级版,进程和线程都面临着内核态和用户态的切换问题而耗费许多切换时间,而协程就是用户自己控制切换的时机,不再需要陷入系统的内核态. 20. 闭包 闭包(closure)是函数式编程的重要的语法结构。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。 当一个内嵌函数引用其外部作作用域的变量,我们就会得到一个闭包. 总结一下,创建一个闭包必须满足以下几点: 必须有一个内嵌函数内嵌函数必须引用外部函数中的变量外部函数的返回值必须是内嵌函数感觉闭包还是有难度的,几句话是说不明白的,还是查查相关资料. 重点是函数运行后并不会被撤销,就像16题的instance字典一样,当函数运行完后,instance并不被销毁,而是继续留在内存空间里.这个功能类似类里的类变量,只不过迁移到了函数上. 闭包就像个空心球一样,你知道外面和里面,但你不知道中间是什么样.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"关于celery介绍","slug":"关于celery介绍","date":"2019-02-03T19:04:43.000Z","updated":"2019-02-08T18:10:31.502Z","comments":true,"path":"2019/02/04/关于celery介绍/","link":"","permalink":"http://yoursite.com/2019/02/04/关于celery介绍/","excerpt":"1.Celery是什么 1.1 Celery 是一个由 Python 编写的简单、灵活、可靠的用来处理大量信息的分布式系统,它同时提供操作和维护分布式系统所需的工具(它本身不是一个任务队列， 它是 任务队列管理的工具， 它提供的接口可以帮助我们实现分布式任务队列)。 1.2 Celery 专注于实时任务处理，支持任务调度(跟rabbitMQ可实现多种exchange。) 说白了，它是一个分布式队列的管理工具，我们可以用 Celery 提供的接口快速实现并管理一个分布式的任务队列。","text":"1.Celery是什么 1.1 Celery 是一个由 Python 编写的简单、灵活、可靠的用来处理大量信息的分布式系统,它同时提供操作和维护分布式系统所需的工具(它本身不是一个任务队列， 它是 任务队列管理的工具， 它提供的接口可以帮助我们实现分布式任务队列)。 1.2 Celery 专注于实时任务处理，支持任务调度(跟rabbitMQ可实现多种exchange。) 说白了，它是一个分布式队列的管理工具，我们可以用 Celery 提供的接口快速实现并管理一个分布式的任务队列。 1.3 Celery 架构 消息中间件(message broker)（邮箱， 邮局）: 本身不提供消息服务，可以和第三方消息中间件集成，常用的有 redis mongodb rabbitMQ 任务执行单元(worker)（寄件人）: 是Celery提供的任务执行单元， worker并发的运行在分布式的系统节点中 任务执行结果存储(task result store)（收件人）：用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括Redis，MongoDB，Django ORM，AMQP等 1.4 任务队列和消息队列 任务队列是一种在线或机器分发任务的机制 消息队列输入是工作的一个单元， 可以认为是一个任务，独立的职程（Worker）进程持续监视队列中是否有需要处理的新任务。 图解 2.简单示例2.1 创建一个celery实例 创建tasks.py文件12345678910import timefrom celery import Celeryapp = Celery('tasks', broker='redis:////127.0.0.1:6379/6', backend='redis:////127.0.0.1:6379/7')@app.taskdef add(x, y): time.sleep(10) return x + y ps: tasks为任务名称 设置reids为中间件 2.2 创建一个index.py文件调用并且检测任务、查看任务执行状态123456789101112131415161718192021222324#!/usr/bin/env python# -*- coding:utf-8 -*-from tasks import add, appfrom celery.result import AsyncResultimport time# 立即告知celery去执行add任务，并传入两个参数result = add.delay(4, 4)print(result.id)async = AsyncResult(id=result.id, app=app)time.sleep(3)if async.successful(): result = async.get() print(result, \"执行成功\") # result.forget() # 将结果删除elif async.failed(): print('执行失败')elif async.status == 'PENDING': print('任务等待中被执行')elif async.status == 'RETRY': print('任务异常后正在重试')elif async.status == 'STARTED': print('任务已经开始被执行') ps 如果使用redis作为任务队列中间人，在redis中存在两个键 celery 和 _kombu.binding.celery ， _kombu.binding.celery 表示有一名为 celery 的任务队列（Celery 默认），而 celery为默认队列中的任务列表，使用list类型，可以看看添加进去的任务数据。 2.3 执行命令详解 celery -A app.celery_tasks.celery worker -Q queue –loglevel=info A参数指定celery对象的位置，该app.celery_tasks.celery指的 是app包下面的celery_tasks.py模块的celery实例，注意一定是初始化后的实例， Q参数指的是该worker接收指定的队列的任务，这是为了当多个队列有不同的任务时可以独立；如果不设会接收所有的队列的任务； l参数指定worker的日志级别； 执行完毕后结果存储在redis中，查看redis中的数据，发现存在一个string类型的键值对celery-task-meta-064e4262-e1ba-4e87-b4a1-52dd1418188f:data该键值对的失效时间为24小时 2.4 消息主体分析 body : 是序列化后使用base64编码的信息，包括具体的任务参数，其中包括了需要执行的方法、参数和一些任务基本信息 content-encoding: 序列化数据编码方式 content-type: 任务数据的序列化方式，默认使用python内置的序列化模块pickle(ps: pickle模块支持的类型 所有python支持的原生类型：布尔值，整数，浮点数，复数，字符串，字节，None。由任何原生类型组成的列表，元组，字典和集合。函数，类，类的实例， 常用的方法：dumps,dump,loads,load) 123456789101112131415161718&#123; &quot;body&quot;: &quot;gAJ9cQAoWAQAAAB0YXNrcQFYCQAAAHRhc2tzLmFkZHECWAIAAABpZHEDWCQAAABjNDMwMzZkMi03Yzc3LTQ0MDUtOTYwNC1iZDc3ZTcyNzNlN2FxBFgEAAAAYXJnc3EFSwRLBIZxBlgGAAAAa3dhcmdzcQd9cQhYBwAAAHJldHJpZXNxCUsAWAMAAABldGFxCk5YBwAAAGV4cGlyZXNxC05YAwAAAHV0Y3EMiFgJAAAAY2FsbGJhY2tzcQ1OWAgAAABlcnJiYWNrc3EOTlgJAAAAdGltZWxpbWl0cQ9OToZxEFgHAAAAdGFza3NldHERTlgFAAAAY2hvcmRxEk51Lg==&quot;, &quot;content-encoding&quot;: &quot;binary&quot;, &quot;content-type&quot;: &quot;application/x-python-serialize&quot;, &quot;headers&quot;: &#123;&#125;, &quot;properties&quot;: &#123; &quot;reply_to&quot;: &quot;caa78c3a-618a-31f0-84a9-b79db708af02&quot;, &quot;correlation_id&quot;: &quot;c43036d2-7c77-4405-9604-bd77e7273e7a&quot;, &quot;delivery_mode&quot;: 2, &quot;delivery_info&quot;: &#123; &quot;priority&quot;: 0, &quot;exchange&quot;: &quot;celery&quot;, &quot;routing_key&quot;: &quot;celery&quot; &#125;, &quot;body_encoding&quot;: &quot;base64&quot;, &quot;delivery_tag&quot;: &quot;e7e288b5-ecbb-4ec6-912c-f42eb92dbd72&quot; &#125;&#125; 2.5 Celery配置1234567CELERY_DEFAULT_QUEUE：默认队列BROKER_URL : 代理人的网址CELERY_RESULT_BACKEND：结果存储地址CELERY_TASK_SERIALIZER：任务序列化方式CELERY_RESULT_SERIALIZER：任务执行结果序列化方式CELERY_TASK_RESULT_EXPIRES：任务过期时间CELERY_ACCEPT_CONTENT：指定任务接受的内容序列化类型(序列化)，一个列表； 2.6 获取执行任务执行结果的方法123456789r = func.delay(...)r.ready() # 查看任务状态，返回布尔值, 任务执行完成, 返回 True, 否则返回 False.r.wait() # 等待任务完成, 返回任务执行结果，很少使用；r.get(timeout=1) # 获取任务执行结果，可以设置等待时间r.result # 任务执行结果.r.state # PENDING, START, SUCCESS，任务当前的状态r.status # PENDING, START, SUCCESS，任务当前的状态r.successful # 任务成功返回truer.traceback # 如果任务抛出了一个异常，你也可以获取原始的回溯信息 2.7 celery的装饰方法celery.task task()把任务（函数）装饰成异步 12345@celery.task()def func(): # do something pass 可以重新定义任务的基类 1234567891011class MyTask(celery.Task): # 任务失败时执行 def on_failure(self, exc, task_id, args, kwargs, einfo): print('&#123;0!r&#125; failed: &#123;1!r&#125;'.format(task_id, exc)) # 任务成功时执行 def on_success(self, retval, task_id, args, kwargs): pass # 任务重试时执行 def on_retry(self, exc, task_id, args, kwargs, einfo): pass 参数 task_id : 任务id einfo：执行失败时任务详情 exc： 失败时的错误类型 retval： 任务成功时返回的执行结果 2.8 一份完整的配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445# 注意，celery4版本后，CELERY_BROKER_URL改为BROKER_URLBROKER_URL = 'amqp://username:passwd@host:port/虚拟主机名'# 指定结果的接受地址CELERY_RESULT_BACKEND = 'redis://username:passwd@host:port/db'# 指定任务序列化方式CELERY_TASK_SERIALIZER = 'msgpack' # 指定结果序列化方式CELERY_RESULT_SERIALIZER = 'msgpack'# 任务过期时间,celery任务执行结果的超时时间CELERY_TASK_RESULT_EXPIRES = 60 * 20 # 指定任务接受的序列化类型.CELERY_ACCEPT_CONTENT = [\"msgpack\"] # 任务发送完成是否需要确认，这一项对性能有一点影响 CELERY_ACKS_LATE = True # 压缩方案选择，可以是zlib, bzip2，默认是发送没有压缩的数据CELERY_MESSAGE_COMPRESSION = 'zlib' # 规定完成任务的时间CELERYD_TASK_TIME_LIMIT = 5 # 在5s内完成任务，否则执行该任务的worker将被杀死，任务移交给父进程# celery worker的并发数，默认是服务器的内核数目,也是命令行-c参数指定的数目CELERYD_CONCURRENCY = 4 # celery worker 每次去rabbitmq预取任务的数量CELERYD_PREFETCH_MULTIPLIER = 4 # 每个worker执行了多少任务就会死掉，默认是无限的CELERYD_MAX_TASKS_PER_CHILD = 40 # 设置默认的队列名称，如果一个消息不符合其他的队列就会放在默认队列里面，如果什么都不设置的话，数据都会发送到默认的队列中CELERY_DEFAULT_QUEUE = \"default\" # 设置详细的队列CELERY_QUEUES = &#123; \"default\": &#123; # 这是上面指定的默认队列 \"exchange\": \"default\", \"exchange_type\": \"direct\", \"routing_key\": \"default\" &#125;, \"topicqueue\": &#123; # 这是一个topic队列 凡是topictest开头的routing key都会被放到这个队列 \"routing_key\": \"topic.#\", \"exchange\": \"topic_exchange\", \"exchange_type\": \"topic\", &#125;, \"task_eeg\": &#123; # 设置扇形交换机 \"exchange\": \"tasks\", \"exchange_type\": \"fanout\", \"binding_key\": \"tasks\", &#125;, &#125; 2.8 Celery定时任务 指定定时任务并加入配置 重新启动worker 123456789101112131415# config.pyfrom datetime import timedeltafrom celery.schedules import crontab CELERYBEAT_SCHEDULE = &#123; 'ptask': &#123; 'task': 'tasks.period_task', 'schedule': timedelta(seconds=5), &#125;,&#125;# 添加定时任务@app.task(bind=True)def period_task(self): print 'period task done: &#123;0&#125;'.format(self.request.id) PS:时间如果涉及到datatime最好设置为UTC时间 启动定时任务进程 1celery -A task beat 2.9 链式任务链式任务就是异步或者定时执行的任务由多个子任务执行完成 123456789101112131415161718def update_page_info(url): # fetch_page -&gt; parse_page -&gt; store_page chain = fetch_page.s(url) | parse_page.s() | store_page_info.s(url) chain() @app.task()def fetch_page(url): return myhttplib.get(url) @app.task()def parse_page(page): return myparser.parse_document(page) @app.task(ignore_result=True)def store_page_info(info, url): PageInfo.objects.create(url=url, info=info) fetch_page.apply_async((url), link=[parse_page.s(), store_page_info.s(url)]) 3 Celery, rabbitmq实现exchange三种模式 celery的工作流程 borker : 消息中间件 worker : 任务执行单元 storgae: 任务执行结果存储 3.1 exchange 常用模式介绍(direct, fanout, topic, header) 当我们执行的任务需要根据特定的需要进行分类时，我们可以对任务创建多个队列进行， 每一个队列交换方式可以指定，需要注意的是：redis只能提供 direct exchange 方式， 也是默认指定的方式，所以我们把中间人换成了rabbitmq。 首先我们来了解一下交换模式有哪些？ Direct Exchange 模式 这种模式是rabbitmq（redis）自带的一种模式，所以我们在实际使用过程中只要指定routing_key就可以了，或者是指定队列名称即可。 ps: 如果我们指定的队列名称不在配置里面，那我们创建的这条消息任务会被自动废除，所以需要检查下配置里的队列是否正确，因为rabbitmq只具备存储队列的能力，不能存储消息信息。 Fanout Exchange 模式 fanout模式不用指定routing_key， 所有exchange_ type 是fanout的 Queue都会执行 所以队列和fanout可以多对多绑定。 Topic Exchange 模式 任何发送到Topic Exchange的消息都会被转发到所有关心RouteKey中指定话题的Queue上 这种模式较为复杂，简单来说，就是每个队列都有其关心的主题，所有的消息都带有一个“标题”(RouteKey)，Exchange会将消息转发到所有关注主题能与RouteKey模糊匹配的队列。 这种模式需要RouteKey，也许要提前绑定Exchange与Queue。 在进行绑定时，要提供一个该队列关心的主题，如“#.log.#”表示该队列关心所有涉及log的消息(一个RouteKey为”MQ.log.error”的消息会被转发到该队列)。 “#”表示0个或若干个关键字，“”表示一个关键字。如“log.”能与“log.warn”匹配，无法与“log.warn.timeout”匹配；但是“log.#”能与上述两者匹配。 同样，如果Exchange没有发现能够与RouteKey匹配的Queue，则会抛弃此消息。 3.2 在实际例子中如何去运用这三种模式 首先要安装rabitmq并且启动 rabbitmq-server 创建rabbitmq_config.py 文件， 并且把之前在tasks.py中引用的配置修改为rabbitmq_config，代码如下 1234567891011121314#coding:utf-8from celery.schedules import crontabimport sysimport ossys.path.insert(0, os.getcwd())CELERY_IMPORTS = (&quot;tasks&quot;, )CELERY_RESULT_BACKEND = &quot;amqp&quot;BROKER_HOST = &quot;localhost&quot;BROKER_PORT = 5672BROKER_USER = &quot;guest&quot;BROKER_PASSWORD = &quot;guest&quot;BROKER_VHOST = &quot;/&quot; 创建需要的交换方式 123456789101112131415161718default_exchange = Exchange('dedfault', type='direct')# 定义一个媒体交换机,类型是直连交换机media_exchange = Exchange('media', type='direct')# 定义一个image交换机,类型是fanout交换机image_exchange = Exchange('media', type='direct')# 创建三个队列，一个是默认队列，一个是video、一个imageCELERY_QUEUES = ( Queue('default', default_exchange, routing_key='default'), Queue('videos', media_exchange, routing_key='media.video'), Queue('images', media_exchange, routing_key='media.image'))# 定义默认队列和默认的交换机routing_keyCELERY_DEFAULT_QUEUE = 'default'CELERY_DEFAULT_EXCHANGE = 'default'CELERY_DEFAULT_ROUTING_KEY = 'default' 在tasks.py中指定任务 123456789101112131415161718192021222324252627282930# 视频压缩@app.taskdef video_compress(video_name): time.sleep(10) print('Compressing the:', video_name) return 'success'@app.taskdef video_upload(video_name): time.sleep(5) print( u'正在上传视频') return 'success'# 压缩照片@app.taskdef image_compress(image_name): time.sleep(10) print('Compressing the:', image_name) return 'success'# 其他任务@app.taskdef other(str): time.sleep(10) print ('Do other things') return 'success' 指定路由 1234567891011CELERY_ROUTES = (&#123;'tasks.image_compress': &#123; 'queue': 'images', 'routing_key': 'media.image' &#125;&#125;,&#123;'tasks.video_upload': &#123; 'queue': 'videos', 'routing_key': 'media.video' &#125;&#125;,&#123;'tasks.video_compress': &#123; 'queue': 'videos', 'routing_key': 'media.video' &#125;&#125;, ) 现在执行创建的任务在启动worker的时候可以分两种启动方式 第一种： 指定Queue 第二种 ： 不指定（全部执行） ps 为了更好的看到我们添加的队列，还有相应的交换模式，启动全部的队列 启动worker [queue]中包含了创建的队列，其他参数本文前面可以对照[tasks]中显示了我们所有的任务 123456789101112131415161718192021222324252627---- **** ----- --- * *** * -- Darwin-18.2.0-x86_64-i386-64bit 2018-12-28 15:38:00-- * - **** --- - ** ---------- [config]- ** ---------- .&gt; app: tasks:0x104e78d68- ** ---------- .&gt; transport: amqp://guest:**@localhost:5672//- ** ---------- .&gt; results: amqp://- *** --- * --- .&gt; concurrency: 4 (prefork)-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)--- ***** ----- -------------- [queues] .&gt; default exchange=dedfault(direct) key=default .&gt; images exchange=media(direct) key=media.image .&gt; others exchange=other(fanout) key=other.others .&gt; videos exchange=media(direct) key=media.video[tasks] . tasks.add . tasks.dr . tasks.image_compress . tasks.other . tasks.period_task . tasks.task . tasks.video_compress . tasks.video_upload[2018-12-28 15:38:00,906: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672// 执行结果(可以在rabbimq后台管理中看相关执行结果)： 12345678910111213[2018-12-28 15:38:00,906: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//[2018-12-28 15:38:00,933: INFO/MainProcess] mingle: searching for neighbors[2018-12-28 15:38:02,013: INFO/MainProcess] mingle: all alone[2018-12-28 15:38:02,091: INFO/MainProcess] celery@zhanlingjiedeMacBook-Pro.local ready.[2018-12-28 15:38:42,386: INFO/MainProcess] Received task: tasks.add[1fdfbc23-e106-49ab-ac25-d46c2b5e8960] [2018-12-28 15:38:42,429: INFO/ForkPoolWorker-3] Task tasks.add[1fdfbc23-e106-49ab-ac25-d46c2b5e8960] succeeded in 0.040455893002217636s: 5[2018-12-28 15:38:46,397: INFO/MainProcess] Received task: tasks.image_compress[cab797c5-eaae-4f11-b55c-041f4256ead9] [2018-12-28 15:38:46,410: INFO/MainProcess] Received task: tasks.other[0b00fd52-2251-42ef-9743-49df3f2906ed] [2018-12-28 15:38:56,401: WARNING/ForkPoolWorker-4] Compressing the:[2018-12-28 15:38:56,402: WARNING/ForkPoolWorker-4] 这是我上传的图片[2018-12-28 15:38:56,412: WARNING/ForkPoolWorker-3] Do other things[2018-12-28 15:38:56,447: INFO/ForkPoolWorker-3] Task tasks.other[0b00fd52-2251-42ef-9743-49df3f2906ed] succeeded in 10.036200570997607s: 'success'[2018-12-28 15:38:56,461: INFO/ForkPoolWorker-4] Task tasks.image_compress[cab797c5-eaae-4f11-b55c-041f4256ead9] succeeded in 10.061314186998061s: 'success' 补充一般在使用celery为了和实际场景结合会使用框架去使用 django + celery + redis(rabbitmq)","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"django-celery","slug":"django-celery","permalink":"http://yoursite.com/tags/django-celery/"}]},{"title":"Continue writing","slug":"Continue-writing","date":"2019-02-03T18:57:16.000Z","updated":"2019-02-03T19:02:47.375Z","comments":true,"path":"2019/02/04/Continue-writing/","link":"","permalink":"http://yoursite.com/2019/02/04/Continue-writing/","excerpt":"","text":"从2017-8开始写自己的博客，主要是想对自己学习的东西做一个记录，为了方便后期解决问题查阅 经过两年的多的时间已经养成了习惯，与其说是博客用笔记更为恰当 很可惜在前段时间电脑重装了下，仓库也没有备份，写一篇继续篇再次记录自己的学习路程 开始于2019-2-4 大年三十","categories":[{"name":"随记","slug":"随记","permalink":"http://yoursite.com/categories/随记/"}],"tags":[{"name":"feelings","slug":"feelings","permalink":"http://yoursite.com/tags/feelings/"}]}]}