{"meta":{"title":"詹灵杰博客","subtitle":"詹灵杰博客","description":"当你真心渴望追求某种事物的话，整个宇宙都会联合起来帮你完成。  ——《牧羊少年奇幻之旅》","author":"zhanlingjie","url":"https://blog.jayzhan.cn"},"pages":[{"title":"categories","date":"2019-02-03T18:50:50.000Z","updated":"2019-02-03T18:50:50.899Z","comments":true,"path":"categories/index.html","permalink":"https://blog.jayzhan.cn/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-02-03T17:43:15.000Z","updated":"2019-06-05T02:06:54.253Z","comments":true,"path":"about/index.html","permalink":"https://blog.jayzhan.cn/about/index.html","excerpt":"","text":"詹灵杰 python研发工程师 人生信条：责则于主 喜欢民谣、指弹吉他、有银子的时候也喜欢去旅游、耳机发烧友 一直在内向区徘徊，最近迷上了看散文，喝茶"},{"title":"tags","date":"2019-02-03T18:09:37.000Z","updated":"2019-02-03T18:09:37.281Z","comments":true,"path":"tags/index.html","permalink":"https://blog.jayzhan.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"mysql双机热备实现","slug":"mysql双机热备实现","date":"2019-06-11T03:16:37.000Z","updated":"2019-06-11T08:33:51.474Z","comments":true,"path":"2019/06/11/mysql双机热备实现/","link":"","permalink":"https://blog.jayzhan.cn/2019/06/11/mysql双机热备实现/","excerpt":"在项目部署上线的时候，除了bug的修复最重要的还是数据的安全，为了防止一些特殊情况的发生，我们会采用一些容灾备份的方案，在正常运行的情况下也需要对数据库进行备份， 相对于冷备份，热备份和热拷贝实时性是比较高的。有时候为了提供数据库的性能也会读写分离，实际方案也有很多种，比如master-slave备份机制， master-master 备份机制等等。","text":"在项目部署上线的时候，除了bug的修复最重要的还是数据的安全，为了防止一些特殊情况的发生，我们会采用一些容灾备份的方案，在正常运行的情况下也需要对数据库进行备份， 相对于冷备份，热备份和热拷贝实时性是比较高的。有时候为了提供数据库的性能也会读写分离，实际方案也有很多种，比如master-slave备份机制， master-master 备份机制等等。 主从备份 准备工作 主机A : 39.96.56.70 (master) 主机B : 23.95.229.223 (slave) 说明 把主机A当成master机器， 主机B为salve机器，利用mysql自带的 REPLICATION完成热备份，那么当在主机A中有数据改变的时候会同步到主机B中，所以可以把主机A当成write database， 主机B为read database，可以针对操作不同比如选择指定的数据库引擎，还有可以选择相关的锁机制。 配置基本表：分别在两台主机上安装mysql，并且指定ip开通远程连接，创建需要备份的数据库以及创建一张测试表 123mysql&gt; create database hotcopytest;mysql&gt; use person;mysql&gt; create tables(name varchar(20) comment'name'); 主机A配置创建同步用户(user replicate)12mysql&gt; grant replication slave on *.* to 'replicate'@'23.95.229.223' identified by '123456';mysql&gt; flush privileges; 创建完之后,在slave中登录 1ubuntu: mysql -h23.95.229.223 -ureplicate -p123456 修改mysql 配置文件Linux中mysql的配置文件在 /etc/mysql/my.cnf, 如果my.cnf中引了con.d文件夹的话可以在该文件夹下进行配置, 打开配置文件后在 [mysqld]下修改 server-id只要唯一自定义就可以 log_bin 可以自定义bin-log日志 max_bin_log_szie 为bin_log日志的容量 binlog_d0_db 需要备份的数据库 binlog_ignore_db 被忽略的数据库 配置完了之后重启主机A，查看数据库状态（ 在生产环境下记得先锁表！！！） 主机B配置同样修改配置文件并重启数据库 用change master 语句指定同步位置123456mysql&gt; change master to mysql&gt;master_host='23.95.229.223',master_user='replicate',master_password='123456',mysql&gt; master_log_file=' mysql-bin.000056 ',master_log_pos=154;mysql&gt; stop salve; # 重新设置salve， salve线程重启mysql&gt; reset slave;mysql&gt; start slave; 查看slave服务器状态查看到Salv_IO_Running = Yes 查看到Salv_SQL_Running = Yes 测试同步在主机A中表插入数据， 主机B自动同步则成功了。","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.jayzhan.cn/tags/mysql/"}]},{"title":"2019-6-5阶段总结","slug":"2019-6-5阶段总结","date":"2019-06-05T02:02:39.000Z","updated":"2019-06-05T03:34:44.330Z","comments":true,"path":"2019/06/05/2019-6-5阶段总结/","link":"","permalink":"https://blog.jayzhan.cn/2019/06/05/2019-6-5阶段总结/","excerpt":"现在我更加坚信在生活的道路上并没有多少捷径， 所以呢革命仍未成功，同志仍需努力，仔细算下来，其实自己弹吉他的时间已经寥寥无几了，这是不是走向成熟的标志之一呢，哈哈？","text":"现在我更加坚信在生活的道路上并没有多少捷径， 所以呢革命仍未成功，同志仍需努力，仔细算下来，其实自己弹吉他的时间已经寥寥无几了，这是不是走向成熟的标志之一呢，哈哈？ &#160; &#160; &#160; &#160;2019年第一篇阶段性总结，从今年开始才发现去年一年在公司安心的写着CRUD,一直待在舒适区，并没有在理论上有很多的提高。重度使用django+ restframework， 纯后端的研发让我对于一些前端的布局组件的内容有些淡忘了，现在接触到一些支付宝小程序的前端， 才逐渐的把es6语法熟悉起来，之前接的外包项目虽然可以在一个星期内改完后台管理的页面，但是对前端的认知还仅仅停留在表面，在原生js只停留在了解， 用vue框架太多。 &#160; &#160; &#160; &#160;之前在海康、阿里的面试中可以总结出自己对于手写sql这一块还很薄弱， 习惯使用了python中的ORM模块之后对于原生sql还不能融会贯通，所以后期这一块是要后期的加强，现在阶段尽量在一些数据操作中尽量使用原生的sql。另外一块就是高并发分布式存储数据一致性的问题，这个问题在单体应用的开发中运用到的并不多， 但是随着业务量和用户量的增加，对于业务拆分更加细致，大公司现在已经后端已经都向微服务架构靠拢，python在这一块并没有成熟的框架，之前看了一个框架nameko，这个框架是从一篇博客中看的，对于直接的api的封装调用或者启用rpc进行调用都可以作为网关使用， 那么根据业务拆分成不同的单体应用， django本身整合的会比较多，框架中的有些东西用到的都不多，这个时候可以使用flask或者tornado进行业务拆分，然后在注册服务中心进行统一部署，这是我对于微服务架构的理解， 不同的单体之间还是通过api来通信， 或者是用一些消息机制，在分布式里面对于日志的追踪用消息机制偏多，大公司往往会创建一个可视化的web用来专门监听此类事件。所以对于自己而言，如果使用python想做更多的事情，还是要熟练使用很多的框架。 &#160; &#160; &#160; &#160;大学的时候主修了数电、模电、通信原理相关的知识，但是对于底层通信这一块已经忘记的差不多了，这两年做的事情都是和http打交道，对于http2.0的一些属性也不是很了解，以前考试的时候动不动就要我们写OSI，实际的工作范围一直停留在应用层，所以之前世界读书日的时候专门买了两本书讲Tcp和Http的相关内容，希望自己在通信这块有更深入的了解吧。其次还有一块是数据结构与算法， 说实话在工作当中语言所提供的ADT类型已经满足90%以上的需求， 但是有时候对于一些大数据的分析优化还是需要对数据结构不断的优化，合理运用数据结构对于空间和时间复杂度都会有显著的提升，现在对于线性结构已经了解的差不多了，这也是目前这一个月看的最多的地方，接下来再花一个月的时间去把树这一块搞定， 然后去刷《剑指offer》，其实之前在leetcode上也刷了几十道题，但是基本也是属于写完就忘的阶段， 所以还是以面试为主去看一些算法吧，等到自己稳定下来了再把所有的体系都再看一遍。 &#160; &#160; &#160; &#160; 这三个月的计划是 把数据结构与算法看完，顺便把赵海英老师的视频也看一遍，把书本后面的题目加上《剑指offer》。 另一块是熟练手写sql， 现在发现如果单纯的看一本书会变得枯燥无味，所以接下来打算两三本书轮流看，也算是解除疲劳的一种方式吧 &#160; &#160; &#160; &#160; 今年开始我对于工作越来越习惯了，甚至说有些享受，因为一切都按照我既定的目标在走，虽然心里还是会有一点点小小的急躁，功到自然成，还是要宽慰下自己，还有一件事情就是我健身也快坚持一个月了，希望自己能继续坚持每天10公里的运动量，健康的体魄也还是尤为重要的。虽然离实现财务自由还很远，给自己一点时间， 我相信自己肯定能变的更好，加油。","categories":[],"tags":[{"name":"随记","slug":"随记","permalink":"https://blog.jayzhan.cn/tags/随记/"}]},{"title":"Josephus问题多重解","slug":"Josephus问题多重解","date":"2019-05-30T03:10:32.000Z","updated":"2019-06-04T11:09:55.841Z","comments":true,"path":"2019/05/30/Josephus问题多重解/","link":"","permalink":"https://blog.jayzhan.cn/2019/05/30/Josephus问题多重解/","excerpt":"问题描述:Josephubs(约瑟夫环)：假设有n个人围坐一圈， 现在要求从k个人开始报数， 报到第m个数的人退出。然后从下一个人开始继续报数并按同样规则退出，直到所有人退出。要求按顺序输出各出列人的编号","text":"问题描述:Josephubs(约瑟夫环)：假设有n个人围坐一圈， 现在要求从k个人开始报数， 报到第m个数的人退出。然后从下一个人开始继续报数并按同样规则退出，直到所有人退出。要求按顺序输出各出列人的编号 问题分析: 初始 建立一个包含n个人(的编号)的表。 找到第k个人， 那里开始。 处理过程中采用把相应表元素修改为0的方式表示已出列， 反复做: 数m个人(尚在座的人)，遇到表的末端就转回下标0继续。 把表示第m个人的 表元素修改为0。 n个人出列即结束 采用python自带的list ADT模块思路描述：创建一个list， 当第m个位置被排出的时候该元素位置为0， 那么下次继续设置初始下标为 i+1 % n , count用于统计， 确定在排出所有空位置之后的真实第m个位置。 123456789101112131415161718192021def josephus(n, k, m): people_num = list(range(1, n + 1)) print(people_num) i = k - 1 # 第k个人的下标值 for num in range(n): count = 0 # 用于计算是第几个人 while count &lt; m: if people_num[i] != 0: count += 1 if m == count: print(\"找到该同学为:&#123;&#125;\".format(people_num[i])) people_num[i] = 0 i = (i+1) % n # 下标反转， 第二次查找时由于前面排出一个同学，所以需要+1 if num &lt; n-1: print('第&#123;&#125;次查找结束'.format(num)) else: print('结束') 时间复杂度分析 当m=1的时候， 内循环每次只要执行一次，所以内循环的时间复杂度为O(1), 所以综合时间度为O(n) 当 m = n时，那么内循环需要把整个表遍历一遍，之后需要执行 i = (i+1) % n 语句,所以它的时间复杂度为O(n2)。 基于顺序表的解设计思路：用python list 的pop取出需要元素， 重新计算i值， 12345678def josephus1(n, k, m): people_num = list(range(1, n + 1)) num, i = n, k-1 for num in range(n, 0, -1): # 倒排 i = (i + m - 1) % num print('第&#123;&#125;同学被排出'.format(people_num.pop(i))) 时间复杂度分析 该算法用了一个for循环，时间复杂度为O(n), python list pop()方法也是O(n),所以该方法的综合时间复杂度还是O(n2) 基于循环单链表的解设计思路： 把围坐一圈的人当成循环链表， 通过next连接，先到初始第k后第m个next时， 删除该节点， 因为是循环节点， 所以可以按照节点顺序依次执行下去， 直到删除最后一个元素 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 首先要创建一个简单的链表节点class LNode(object): def __init__(self, elem, next_=None) self.elem = elem self.next = next_# 循环单链表基类class LClist(objects): def __init__(self): self._rear = None def is_empty(self): return self._rear is None def prepend(self, elem): \"\"\" 从头部加入 \"\"\" p = LNode(elem) if self._rear is None: p.next = p # 单节点组成环 self._rear.next = p else: p.next = self._rear.next self._rear.next = p def append(self, elem): \"\"\" 从尾部插入 \"\"\" self. prepend(elem) self._rear = self._rear.next def pop(self): \"\"\" 头部弹出 \"\"\" if self._rear._rear is None: return False p = self._rear.next: if self._reear is p: self._rear = None else: self._rear = None return p.elem class Josehups(LCList): def turn(self, m): for i in range(m): self._rear = self._rear.next def __init__(self, n, k, m): LCList.__init__(self) for i in range(n): self.append(i+1) self.turn(k-1) while not self.is_empty(): self.turn(m-1) print(self.pop() if self.is_empty() else \"\") 时间复杂度分析 初建表的复杂度为O(n), 后面循环的算法复杂度为O(m x n)， 每次旋转的时间复杂度为O()","categories":[],"tags":[{"name":"-algorithm","slug":"algorithm","permalink":"https://blog.jayzhan.cn/tags/algorithm/"}]},{"title":"关于顺序表的理解","slug":"关于顺序表的理解","date":"2019-05-23T09:20:17.000Z","updated":"2019-05-23T10:23:20.187Z","comments":true,"path":"2019/05/23/关于顺序表的理解/","link":"","permalink":"https://blog.jayzhan.cn/2019/05/23/关于顺序表的理解/","excerpt":"关于顺序表的理解 在数据结构中大致分为线性结构和非线性结构，其中我们常用的线性表、队列、栈都是属于线性结构的一种，另外还有图和树是属于非线性结构。线性表又可以分为顺序表和链表，在逻辑结构上顺序表和链表都是一对一关联的线性结构，而顺序表和链表的区别就是顺序表在存储时物理地址是连续性的， 在python中可以用id()查看,而链表在内存中不连续存储，通过元素的前驱和后继来建立元素之间的关系，这样就避免了连续内存块不够用的情况。","text":"关于顺序表的理解 在数据结构中大致分为线性结构和非线性结构，其中我们常用的线性表、队列、栈都是属于线性结构的一种，另外还有图和树是属于非线性结构。线性表又可以分为顺序表和链表，在逻辑结构上顺序表和链表都是一对一关联的线性结构，而顺序表和链表的区别就是顺序表在存储时物理地址是连续性的， 在python中可以用id()查看,而链表在内存中不连续存储，通过元素的前驱和后继来建立元素之间的关系，这样就避免了连续内存块不够用的情况。 优点: 既然顺序表在物理内存中连续存储，那我们就可以根据头结点计算出表中所有元素的位置，例如每个元素所占空间为c， 头结点的内存地址是l, 如果我们要寻找第i个元素时就可以根据公式 r = l + c x (i-1) 得出，或者我们要update一个元素时也可以，所以顺序表在我们读取数据或者修改数据时的效率非常高,其时间复杂度为O(1)。 缺点：那么在实际读取数据过程中顺序表可以很快的定位到某一个元素，但是如果数据有delete/insert操作时其顺序会随着数据量的增加时间开销会非常大，比如在顺序表中在第i个元素之前插入新元素， 那么其之后的每个元素都要往后移动， 也就是说是要循环 data[i+1] = data[i]操作，那么这个时候它的时间复杂度增加到O(n)（最优为O(1)， 所以O(n)是平均时间复杂度，删除也是一样的，所以合理选择顺序表还是比较重要的， 或者说实际语言中的数据结构是 顺序表 + 链表的实现方式。 顺序表类型: 在顺序表初始化中需要定义最大长度使得计算机分配固定的内存块给它， 但是在实际的过程中我们对于数据量没有一个精确的估计，当内存地址分配过少时会出现溢出的情况， 分配过多时会造成内存的浪费， 而且在刚才的查找元素的计算过程中那个我们默认每个元素的空间大小c一定， 在实际应用中每个元素占用的空间肯定不一致， 所以我们对顺序表的结构进行了细分，有一体式结构和分离式结构，一体式结构就是索引和对应的元素值在用一个地址中，所以能够最大化的利用内存空间， 但是对于元素大小不一的用分离式就比较好了， 我们用一张表存储元素索引值， 另一张表存储其真实的值， 很好的保证了其扩容性以及对于元素的查找和更新的优化,在python中的list数据类型就采用了这种结构实现。list就是一种采用分离式技术实现的动态顺序表，其性质都源于这种实现方式。Python的list采用了前面介绍的元素存储区调整策略，如果需要反复加入元素，用lst.insert(len(lst),x)比在一般位置插入的效率高。 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class SeqList(object): \"\"\" 顺序表的实现 \"\"\" def __init__(self, size): ''' size 为顺序表的最大长度 ''' self.max = size self.num = 0 self.data = [None]*self.max def is_empty(self): ''' 判断顺序表是否为空 ''' return self.num is 0 def is_full(self): ''' 判断顺序表是否溢出 ''' return self.max is self.num def __getitem__(self, item): ''' 获取某一位置的值 ''' if not isinstance(item, int): raise Exception('item type error') if 0 &lt;= item &lt;= self.max: return self.data[item] def locate_item(self, value): ''' 按值查找该元素的索引 ''' for i in range(len(self.data)): if self.data[i] == value: return i+1 return 'not found' def count(self): ''' 表元素个数 ''' return self.num def insert(self, index, value): ''' 在特定位置插入某一个元素 ''' if not isinstance(index, int): raise Exception('type error') if index &gt; self.num: raise Exception('list is full') for i in range(self.num, index, -1): self.data[i] = self.data[i - 1] self.data[index] = value self.num += 1 def append_last(self, value): ''' 尾部插入数据''' insert_index = 0 for i in range(self.max): if self.data[i]: insert_index = i if insert_index &lt; self.max: self.data[insert_index+1] = value self.num += 1 else: raise Exception('fulled') def remove(self, index): ''' 移除特定位置的元素 ''' if index &gt; self.max-1: raise Exception('index full') for i in range(index, self.num): self.data[i] = self.data[i+1] self.num -= 1 def print_list(self): ''' 打印顺序表 ''' return self.data","categories":[],"tags":[{"name":"DS","slug":"DS","permalink":"https://blog.jayzhan.cn/tags/DS/"}]},{"title":"python_rabbitmq_实例","slug":"python-rabbitmq-实例","date":"2019-04-29T08:21:56.000Z","updated":"2019-04-29T08:28:54.939Z","comments":true,"path":"2019/04/29/python-rabbitmq-实例/","link":"","permalink":"https://blog.jayzhan.cn/2019/04/29/python-rabbitmq-实例/","excerpt":"解耦：在项目启动之初是很难预测未来会遇到什么困难的，消息中间件在处理过程中插入了一个隐含的，基于数据的接口层，两边都实现这个接口，这样就允许独立的修改或者扩展两边的处理过程，只要两边遵守相同的接口约束即可。 冗余（存储）：在某些情况下处理数据的过程中会失败，消息中间件允许把数据持久化知道他们完全被处理 扩展性：消息中间件解耦了应用的过程，所以提供消息入队和处理的效率是很容易的，只需要增加处理流程就可以了。 削峰：在访问量剧增的情况下，但是应用仍然需要发挥作用，但是这样的突发流量并不常见。而使用消息中间件采用队列的形式可以减少突发访问压力，不会因为突发的超时负荷要求而崩溃 可恢复性：当系统一部分组件失效时，不会影响到整个系统。消息中间件降低了进程间的耦合性，当一个处理消息的进程挂掉后，加入消息中间件的消息仍然可以在系统恢复后重新处理 顺序保证：在大多数场景下，处理数据的顺序也很重要，大部分消息中间件支持一定的顺序性 缓冲：消息中间件通过一个缓冲层来帮助任务最高效率的执行 异步通信：通过把把消息发送给消息中间件，消息中间件并不立即处理它，后续在慢慢处理。","text":"解耦：在项目启动之初是很难预测未来会遇到什么困难的，消息中间件在处理过程中插入了一个隐含的，基于数据的接口层，两边都实现这个接口，这样就允许独立的修改或者扩展两边的处理过程，只要两边遵守相同的接口约束即可。 冗余（存储）：在某些情况下处理数据的过程中会失败，消息中间件允许把数据持久化知道他们完全被处理 扩展性：消息中间件解耦了应用的过程，所以提供消息入队和处理的效率是很容易的，只需要增加处理流程就可以了。 削峰：在访问量剧增的情况下，但是应用仍然需要发挥作用，但是这样的突发流量并不常见。而使用消息中间件采用队列的形式可以减少突发访问压力，不会因为突发的超时负荷要求而崩溃 可恢复性：当系统一部分组件失效时，不会影响到整个系统。消息中间件降低了进程间的耦合性，当一个处理消息的进程挂掉后，加入消息中间件的消息仍然可以在系统恢复后重新处理 顺序保证：在大多数场景下，处理数据的顺序也很重要，大部分消息中间件支持一定的顺序性 缓冲：消息中间件通过一个缓冲层来帮助任务最高效率的执行 异步通信：通过把把消息发送给消息中间件，消息中间件并不立即处理它，后续在慢慢处理。 服务端实例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 import pikaimport loggingimport datetimelogger = logging.getLogger(__name__)class MqServer(object): \"\"\" mq服务端 \"\"\" def __init__(self, host, port, username, password): \"\"\" 初始化数据 :param host: 主机地址 :param port: 端口 :param username: 用户名 :param password: 密码 \"\"\" self.host = host self.port = port self.username = username self.password = password def create_conn(self): \"\"\" 创建连接 \"\"\" try: credential = pika.PlainCredentials(self.username, self.password) conn = pika.BlockingConnection(pika.ConnectionParameters(self.host, self.port, '/', credential)) return conn except Exception as e: logging.warning(\"&#123;&#125;, create mq failed, &#123;&#125;\".format(datetime.datetime.now(), e)) def create_channel(self, con, queue, routing_key, body): \"\"\" :param con: 连接obj :param queue: 队列名 :param routing_key: routing_key :param body: 消息主体 :return: \"\"\" self.channel = con.channel() self.channel.queue_declare(queue=queue) # delivery_mode 参数 2 保证队列中消息和任务也持久化 / 1 是非持久化 self.channel.basic_publish(exchange='', routing_key=routing_key, body=body, properties=pika.BasicProperties( delivery_mode=2 # 使消息或任务也持久化存储 )) def create_fanout_channel(self, conn, message): \"\"\" 创建广播模式频道 \"\"\" self.channel = conn.channel() # 定义交换机 self.channel.exchange_declare(exchange='logs_fanout', exchange_type='fanout') # 将消息发送到交换机, fanout模式下不用申明routing_key self.channel.basic_publish(exchange='logs_fanout', routing_key='', body=message) def create_direct_channel(self, conn, message, rounting_key): \"\"\" 根据routing_key分发消息 \"\"\" self.channel = conn.channel() # 定义交换机名称类型 self.channel.exchange_declare(exchange='direct_test', exchange_type='direct') # 发布消息到交换机direct_test, 且发布的消息携带的关键字rounting_key 是info self.channel.basic_publish(exchange='direct_test', routing_key=rounting_key, body=message) def close(self): \"\"\" 关闭频道 \"\"\" self.channel.close()if __name__ == \"__main__\": mq_server = MqServer('localhost', 5672, 'guest', 'guest') conn = mq_server.create_conn() for i in range(10): mq_server.create_fanout_channel(conn, 'he34567llo') mq_server.create_direct_channel(conn, '你好呀&#123;&#125;'.format(str(i)), 'haha') mq_server.close() 客户端实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596 import pikaimport datetimeclass MqClient(object): \"\"\" mq客户端 \"\"\" def __init__(self, host, port, username, password): \"\"\" 初始化 \"\"\" self.host = host self.port = port self.username = username self.password = password def create_conn(self): \"\"\" 创建连接 \"\"\" try: credential = pika.PlainCredentials(self.username, self.password) conn = pika.BlockingConnection(pika.ConnectionParameters(self.host, self.port, '/', credential)) return conn except Exception as e: print(\"&#123;&#125;, create mq failed, &#123;&#125;\".format(datetime.datetime.now(), e)) def create_channel(self, queue): \"\"\" 创建连接 \"\"\" connection = self.create_conn() # 声明消息队列，消息将在这个队列中进行传递。如果队列不存在，则创建 channel = connection.channel() # 声明消息队列，消息将在这个队列中进行传递。如果队列不存在，则创建 channel.queue_declare(queue=queue, durable=True) # 告诉rabbitmq使用callback来接收信息 channel.basic_consume(queue, on_message_callback=self.call, auto_ack=False) # no_ack=True表示在回调函数中不需要发送确认标识 print(' [*] Waiting for messages. To exit press CTRL+C') # 开始接收信息，并进入阻塞状态，队列里有信息才会调用callback进行处理。按ctrl+c退出。 channel.start_consuming() def create_fanout_conn(self): \"\"\" 创建广播模式\"\"\" connection = self.create_conn() channel = connection.channel() channel.exchange_declare('logs_fanout', exchange_type='fanout') # 创建随机队列, exclusive=True表示建立临时队列， 当consume关闭后，该队列就会被删除 result = channel.queue_declare(exclusive=True) queue_name = result.method.queue # 将队列和exchange绑定 channel.queue_bind(exchange='logs_fanout', queue=queue_name) channel.basic_consume(self.call, queue=queue_name, no_ack=False) print('进入消息等待状态。。。') # 开始接收信息，并进入阻塞状态，队列里有信息才会调用callback进行处理。按ctrl+c退出。 channel.start_consuming() def create_direct_conn(self): \"\"\"\" 创建routing_key \"\"\" connection = self.create_conn() channel = connection.channel() channel.exchange_declare('direct_test', exchange_type='direct') result = channel.queue_declare(exclusive=True) queue_name = result.method.queue # 生成随机队列 severities = ['error', 'haha'] for severity in severities: channel.queue_bind(exchange='direct_test', queue=queue_name, routing_key=severity) channel.basic_consume(self.call, queue=queue_name, no_ack=False) print('进入消息等待状态。。。') # 开始接收信息，并进入阻塞状态，队列里有信息才会调用callback进行处理。按ctrl+c退出。 channel.start_consuming() def create_topic_conn(self): \"\"\" 主题订阅模式 topic模式和direct模式大致相同, 在routing_key中可以用正则去模糊匹配 exchange 模式定义为 direct \"\"\" pass def call(self, ch, method, properties, body): \"\"\" 消息任务回调函数 \"\"\" print(\" [x] Received %r\" % body.decode('utf-8')) import time import random time.sleep(random.random()) # no_ack = False 则开启消息反馈机制 ch.basic_ack(delivery_tag=method.delivery_tag) if __name__ == \"__main__\": mq_client = MqClient('localhost', 5672, 'guest', 'guest') mq_client.create_direct_conn()","categories":[],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"https://blog.jayzhan.cn/tags/rabbitmq/"}]},{"title":"实用的文档","slug":"实用的文档","date":"2019-04-12T08:21:32.000Z","updated":"2019-04-12T08:37:34.823Z","comments":true,"path":"2019/04/12/实用的文档/","link":"","permalink":"https://blog.jayzhan.cn/2019/04/12/实用的文档/","excerpt":"知识方案性文档","text":"知识方案性文档 实用的博客推荐 python 官方文档 socket详解 utiltest 详解 Http 协议详解 100亿条数据处理过程 线程互斥所以及信号量的介绍 django的信号量介绍 python web学习路线 python 对于传值类型的介绍 关于python垃圾回收机制 Tcp k8s django 部署推荐 mysql 索引数据结构以及算法实现 解决跨域请求 vuex 正则表达式 get 和 post的区别 rustful风格 docker 容器文档 docker portainer python I/o多路复用 mysql redis双写一致方案","categories":[],"tags":[{"name":"doc","slug":"doc","permalink":"https://blog.jayzhan.cn/tags/doc/"}]},{"title":"python redis 操作","slug":"python-redis-操作","date":"2019-03-11T02:32:37.000Z","updated":"2019-03-11T02:48:49.828Z","comments":true,"path":"2019/03/11/python-redis-操作/","link":"","permalink":"https://blog.jayzhan.cn/2019/03/11/python-redis-操作/","excerpt":"python 关于redis的常见操作","text":"python 关于redis的常见操作 12345678910111213141516171819202122232425262728293031323334pip install redisimport redis# 创建连接con1 = redis.Redis(host='127.0.0.1', port=6379)# 插入数据# set(name, value, ex=None, px=None, nx=False, xx=False)# 参数 ex，过期时间（秒） px，过期时间（毫秒） nx，如果设置为True，则只有name不存在时，当前set操作才执行 xx，如果设置为True，则只有name存在时，岗前set操作才执行# 批量插入 mset(*args, **kwargs)# 批量获取 mget(keys, *args)# 设置新值并获取原来的值getset(name, value)# 获取子序列， 比如v的特定长度字符getrange(key, start, end)# 设置子序列， 比如v的特定长度字符getrange(key, start, end)# 对key对应值的二进制setbit(name, offset, value)# 对key的值对应的二进制中某位的值getbit(name, offset)","categories":[],"tags":[{"name":"reids","slug":"reids","permalink":"https://blog.jayzhan.cn/tags/reids/"}]},{"title":"python语言特性","slug":"python语言特性","date":"2019-02-12T02:02:16.000Z","updated":"2019-02-12T06:31:07.121Z","comments":true,"path":"2019/02/12/python语言特性/","link":"","permalink":"https://blog.jayzhan.cn/2019/02/12/python语言特性/","excerpt":"工作久了对之前的基础语法做一个整理，以便自己更了解python的语法， 在编码的时候可以更加更加得心应手。","text":"工作久了对之前的基础语法做一个整理，以便自己更了解python的语法， 在编码的时候可以更加更加得心应手。 1. Python函数参数传递 先看两个例子 12345678910111213In [20]: a = 1In [21]: id(a)Out[21]: 4482033600In [22]: def func(a): ...: a = 2 ...: print(id(a), a) ...:In [23]: func(a)4482033632 2In [24]: 所有的变量都可以理解是内存中一个对象的“引用”， 通过id查看变量内存地址 在命名变量a查看的id和在函数中变量a的id是不一样的， 所以作用域的不同会生成两个不同的内存地址1234567891011In [24]: b = 1In [25]: def fun(b): ...: b = 2 ...: print(id(b), b) ...:In [26]: fun(b)4482033632 2In [27]: 在定义b变量之后查看id 和上一个a是类似的关系， 但是当查看全局变量b的时候发现id和第一个例子中函数中a变量的内存地址相同， 所以在python中当值类型相同时， 指针会指向同一个内存地址以减少内存的使用。 2. Python变量 在Python中有常见的string, int, list，float, tuple, dic 其中不可变对象: string, int , float, tuple, 可变对象: list, dictionary 在python中变量存放的是对象引用，虽然对象引用是可变的， 但是对象本身是不变的， 指针不会指向新的内存空间. 3. @staticmethod 和 @classmethod 在python中方法的类型有静态方法, 类方法和实例方法 代码如下: 123456789101112131415161718192021222324252627282930313233343536373839def foo(x): print(x)class A(object): \"\"\" 创建类 \"\"\" def foo(self, x): # 实例方法 ... @classmethod def class_foo(cls, x) # 类方法 ... @staticmethod def static_foo(x): # 静态方法 ...a = A()a.foo(1)A.class_foo(1)A.static(1)a.class_foo(1)a.static(1)``` 创建A类的时候实例方法必须是要创建一个实例a， 并且通过实例a调用实例方法， 但是不能直接用过A类直接调用， 但是实例a通过也可以调用A类的静态方法和类方法。### 4. 类变量和实例变量---类变量: 在创建类的时候定义，并且是所有实例之间共享的值.实例变量: 实例化之后， 每个实例单独拥有的变量，在创建实例中传值 In [40]: class A(object): …: num = 0 …: def init(self, name): …: self.name = name …: A.num += 1 …: print(name, A.num) …: …: …: In [41]: a = A(‘jay’)jay 1 In [42]: b = A(‘jack’)jack 2 In [43]: A.numOut[43]: 2 In [44]: 1234567891011121314### 5.Python自省模式---在定义变量的时候不用声明变量类型， 在程序运行时需要动态获取对象类型,type(),dir(),getattr(),hasattr(),isinstance().### 6.字典推导式和列表推导式---```pythonv = &#123;key: value for (key, value) in iterable &#125;l = [x for x in list] 7.单下划线和双下划线 12345678910In [45]: class B(object): ...: def __init__(self): ...: self.__superprivate = 'hello' ...: self._semprivate = 'word' ...:In [46]: m = B()In [49]: m.__dict__Out[49]: &#123;'_B__superprivate': 'hello', '_semprivate': 'word'&#125; foo类型: python内部函数的命名方式， 用来区别其他用户自定义的命名冲突,常见的init，new等等, _foo: 类的私有变量命名方式，在该类被实例化或者被引用的时候访问不到的变量 foo: 解析器会用_classnamefoo来替代这个名字，以区别和其他类重复命名的冲突，通过对象名._类名__xxx这样的方式可以访问. 8. 迭代器和生成器 在创建列表时，由于受到内存限制， 列表容量有限， 当一次性生成大量数据的列表时会造成大量的内存浪费， 所以在python中采用生成器:边循环边计算的机制 -&gt; generator 1234567&gt;&gt;&gt; L = [x*x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x*x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x0000028F8B774200&gt; 9. * args和**kwargs 用*args和**kwargs只是为了方便并没有强制使用它们. 当你不确定你的函数里将要传递多少参数时你可以用*args.例如,它可以传递任意数量的参数: 1234567891011121314151617181920&gt;&gt;&gt; def print_everything(*args): for count, thing in enumerate(args):... print &apos;&#123;0&#125;. &#123;1&#125;&apos;.format(count, thing)...&gt;&gt;&gt; print_everything(&apos;apple&apos;, &apos;banana&apos;, &apos;cabbage&apos;)0. apple1. banana2. cabbage相似的,**kwargs允许你使用没有事先定义的参数名:&gt;&gt;&gt; def table_things(**kwargs):... for name, value in kwargs.items():... print &apos;&#123;0&#125; = &#123;1&#125;&apos;.format(name, value)...&gt;&gt;&gt; table_things(apple = &apos;fruit&apos;, cabbage = &apos;vegetable&apos;)cabbage = vegetableapple = fruit你也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给*args和**kwargs.命名参数在列表的最前端.例如:def table_things(titlestring, **kwargs) args和**kwargs可以同时在函数的定义中,但是args必须在**kwargs前面. 当调用函数时你也可以用*和**语法.例如: 123456&gt;&gt;&gt; def print_three_things(a, b, c):... print 'a = &#123;0&#125;, b = &#123;1&#125;, c = &#123;2&#125;'.format(a,b,c)...&gt;&gt;&gt; mylist = ['aardvark', 'baboon', 'cat']&gt;&gt;&gt; print_three_things(*mylist)a = aardvark, b = baboon, c = cat 就像你看到的一样,它可以传递列表(或者元组)的每一项并把它们解包.注意必须与它们在函数里的参数相吻合.当然,你也可以在函数定义或者函数调用时用*. 10. 面向切面编程AOP和装饰器 装饰器是一个非常有用的设计， 经常被用于有切面需求的场景, 较为经典的的插入日志，性能测试、事务处理等。可以抽离出大量函数中与函数本身无关的雷同代码病继续重用， 其实就是给函数添加额外功能。 11. 鸭子类型 “当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。” 我们并不关心对象是什么类型，到底是不是鸭子，只关心行为。 比如在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 又比如list.extend()方法中,我们并不关心它的参数是不是list,只要它是可迭代的,所以它的参数可以是list/tuple/dict/字符串/生成器等. 鸭子类型在动态语言中经常使用，非常灵活，使得python不想java那样专门去弄一大堆的设计模式。 12.Python中的重载 之所以要重载， 主要是为了改变可变参数类型或者可变参数个数，面向对象思想了解的情况下很容易理解重载。 13. 新式类和旧式类 这篇文章很好的介绍了新式类的特性: http://www.cnblogs.com/btchenguang/archive/2012/09/17/2689146.html 新式类很早在2.2就出现了,所以旧式类完全是兼容的问题,Python3里的类全部都是新式类.这里有一个MRO问题可以了解下(新式类继承是根据C3算法,旧式类是深度优先),&lt;Python核心编程&gt;里讲的也很多. 一个旧式类的深度优先的例子 12345678910111213141516class A(): def foo1(self): print \"A\"class B(A): def foo2(self): passclass C(A): def foo1(self): print \"C\"class D(B, C): passd = D()d.foo1()# A 按照经典类的查找顺序从左到右深度优先的规则，在访问d.foo1()的时候,D这个类是没有的..那么往上查找,先找到B,里面没有,深度优先,访问A,找到了foo1(),所以这时候调用的是A的foo1()，从而导致C重写的foo1()被绕过 14. new和init的区别 new是一个静态方法,而init是一个实例方法.new方法会返回一个创建的实例,而init什么都不返回.只有在new返回一个cls的实例时后面的init才能被调用.当创建一个新实例时调用new,初始化一个实例时用init. ps: metaclass是创建类时起作用.所以我们可以分别使用metaclass,new和init来分别在类创建,实例创建和实例初始化的时候做一些小手脚. 15. 单例模式 单例模式是一种软件常用的设计模式， 在它核心结构中只包含一个被称为单例类的特殊类。通过单例模式可以保证一个类中只有一个实例并且这个实例易于被外界访问， 从而对实例个数的控制并且节约系统内存资源new()在init()之前被调用，用于生成实例对象。利用这个方法和类的属性的特点可以实现设计模式的单例模式。单例模式是指创建唯一对象，单例模式设计的类只能实例 这个绝对常考啊.绝对要记住1~2个方法,当时面试官是让手写的. 单例模式的实现方法: 使用new方法 1234567891011class Singleton(object): def __new__(cls, *args, **kwargs): if not hasttr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls, *args, **kwargs) return cls._instanceclass MyClass(Singleton): a = 1 共享模式创建实例时把所有实例的dict指向同一个字典， 这样它们具有相同的属性和方法。 12345678910class Borg(object): _state = &#123;&#125; def __new__(cls, *args, **kw): ob = super(Borg, cls).__new__(cls, *args, **kw) ob.__dict__ = cls._state return obclass MyClass2(Borg): a = 1 装饰器版本 1234567891011def singleton(cls): instances = &#123;&#125; def getinstance(*args, **kw): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance@singletonclass MyClass: ... import方法 1234567891011# mysingleton.pyclass My_Singleton(object): def foo(self): passmy_singleton = My_Singleton()# to usefrom mysingleton import my_singletonmy_singleton.foo() Python中的作用域 Python中， 一个变量的作用域总是在代码中被赋值的地方所决定的。 当Python遇到一个变量的话他会按照这样的顺序进行搜索 本地作用域(local) -&gt; 当前作用域被嵌入的本地作用域( Enclosing locals) -&gt; 全局/模块作用域(built-in) 18 GIL线程全局锁 线程全局锁(Global Interpreter Lock),即Python为了保证线程安全而采取的独立线程运行的限制,说白了就是一个核只能在同一时间运行一个线程.对于io密集型任务，python的多线程起到作用，但对于cpu密集型任务，python的多线程几乎占不到任何优势，还有可能因为争夺资源而变慢。 解决办法就是多进程和下面的协程(协程也只是单CPU,但是能减小切换代价提升性能). 19. 协程 简单点说协程是进程和线程的升级版,进程和线程都面临着内核态和用户态的切换问题而耗费许多切换时间,而协程就是用户自己控制切换的时机,不再需要陷入系统的内核态. 20. 闭包 闭包(closure)是函数式编程的重要的语法结构。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。 当一个内嵌函数引用其外部作作用域的变量,我们就会得到一个闭包. 总结一下,创建一个闭包必须满足以下几点: 必须有一个内嵌函数内嵌函数必须引用外部函数中的变量外部函数的返回值必须是内嵌函数感觉闭包还是有难度的,几句话是说不明白的,还是查查相关资料. 重点是函数运行后并不会被撤销,就像16题的instance字典一样,当函数运行完后,instance并不被销毁,而是继续留在内存空间里.这个功能类似类里的类变量,只不过迁移到了函数上. 闭包就像个空心球一样,你知道外面和里面,但你不知道中间是什么样.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://blog.jayzhan.cn/tags/python/"}]},{"title":"关于celery介绍","slug":"关于celery介绍","date":"2019-02-03T19:04:43.000Z","updated":"2019-02-08T18:10:31.502Z","comments":true,"path":"2019/02/04/关于celery介绍/","link":"","permalink":"https://blog.jayzhan.cn/2019/02/04/关于celery介绍/","excerpt":"1.Celery是什么 1.1 Celery 是一个由 Python 编写的简单、灵活、可靠的用来处理大量信息的分布式系统,它同时提供操作和维护分布式系统所需的工具(它本身不是一个任务队列， 它是 任务队列管理的工具， 它提供的接口可以帮助我们实现分布式任务队列)。 1.2 Celery 专注于实时任务处理，支持任务调度(跟rabbitMQ可实现多种exchange。) 说白了，它是一个分布式队列的管理工具，我们可以用 Celery 提供的接口快速实现并管理一个分布式的任务队列。","text":"1.Celery是什么 1.1 Celery 是一个由 Python 编写的简单、灵活、可靠的用来处理大量信息的分布式系统,它同时提供操作和维护分布式系统所需的工具(它本身不是一个任务队列， 它是 任务队列管理的工具， 它提供的接口可以帮助我们实现分布式任务队列)。 1.2 Celery 专注于实时任务处理，支持任务调度(跟rabbitMQ可实现多种exchange。) 说白了，它是一个分布式队列的管理工具，我们可以用 Celery 提供的接口快速实现并管理一个分布式的任务队列。 1.3 Celery 架构 消息中间件(message broker)（邮箱， 邮局）: 本身不提供消息服务，可以和第三方消息中间件集成，常用的有 redis mongodb rabbitMQ 任务执行单元(worker)（寄件人）: 是Celery提供的任务执行单元， worker并发的运行在分布式的系统节点中 任务执行结果存储(task result store)（收件人）：用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括Redis，MongoDB，Django ORM，AMQP等 1.4 任务队列和消息队列 任务队列是一种在线或机器分发任务的机制 消息队列输入是工作的一个单元， 可以认为是一个任务，独立的职程（Worker）进程持续监视队列中是否有需要处理的新任务。 图解 2.简单示例2.1 创建一个celery实例 创建tasks.py文件12345678910import timefrom celery import Celeryapp = Celery('tasks', broker='redis:////127.0.0.1:6379/6', backend='redis:////127.0.0.1:6379/7')@app.taskdef add(x, y): time.sleep(10) return x + y ps: tasks为任务名称 设置reids为中间件 2.2 创建一个index.py文件调用并且检测任务、查看任务执行状态123456789101112131415161718192021222324#!/usr/bin/env python# -*- coding:utf-8 -*-from tasks import add, appfrom celery.result import AsyncResultimport time# 立即告知celery去执行add任务，并传入两个参数result = add.delay(4, 4)print(result.id)async = AsyncResult(id=result.id, app=app)time.sleep(3)if async.successful(): result = async.get() print(result, \"执行成功\") # result.forget() # 将结果删除elif async.failed(): print('执行失败')elif async.status == 'PENDING': print('任务等待中被执行')elif async.status == 'RETRY': print('任务异常后正在重试')elif async.status == 'STARTED': print('任务已经开始被执行') ps 如果使用redis作为任务队列中间人，在redis中存在两个键 celery 和 _kombu.binding.celery ， _kombu.binding.celery 表示有一名为 celery 的任务队列（Celery 默认），而 celery为默认队列中的任务列表，使用list类型，可以看看添加进去的任务数据。 2.3 执行命令详解 celery -A app.celery_tasks.celery worker -Q queue –loglevel=info A参数指定celery对象的位置，该app.celery_tasks.celery指的 是app包下面的celery_tasks.py模块的celery实例，注意一定是初始化后的实例， Q参数指的是该worker接收指定的队列的任务，这是为了当多个队列有不同的任务时可以独立；如果不设会接收所有的队列的任务； l参数指定worker的日志级别； 执行完毕后结果存储在redis中，查看redis中的数据，发现存在一个string类型的键值对celery-task-meta-064e4262-e1ba-4e87-b4a1-52dd1418188f:data该键值对的失效时间为24小时 2.4 消息主体分析 body : 是序列化后使用base64编码的信息，包括具体的任务参数，其中包括了需要执行的方法、参数和一些任务基本信息 content-encoding: 序列化数据编码方式 content-type: 任务数据的序列化方式，默认使用python内置的序列化模块pickle(ps: pickle模块支持的类型 所有python支持的原生类型：布尔值，整数，浮点数，复数，字符串，字节，None。由任何原生类型组成的列表，元组，字典和集合。函数，类，类的实例， 常用的方法：dumps,dump,loads,load) 123456789101112131415161718&#123; &quot;body&quot;: &quot;gAJ9cQAoWAQAAAB0YXNrcQFYCQAAAHRhc2tzLmFkZHECWAIAAABpZHEDWCQAAABjNDMwMzZkMi03Yzc3LTQ0MDUtOTYwNC1iZDc3ZTcyNzNlN2FxBFgEAAAAYXJnc3EFSwRLBIZxBlgGAAAAa3dhcmdzcQd9cQhYBwAAAHJldHJpZXNxCUsAWAMAAABldGFxCk5YBwAAAGV4cGlyZXNxC05YAwAAAHV0Y3EMiFgJAAAAY2FsbGJhY2tzcQ1OWAgAAABlcnJiYWNrc3EOTlgJAAAAdGltZWxpbWl0cQ9OToZxEFgHAAAAdGFza3NldHERTlgFAAAAY2hvcmRxEk51Lg==&quot;, &quot;content-encoding&quot;: &quot;binary&quot;, &quot;content-type&quot;: &quot;application/x-python-serialize&quot;, &quot;headers&quot;: &#123;&#125;, &quot;properties&quot;: &#123; &quot;reply_to&quot;: &quot;caa78c3a-618a-31f0-84a9-b79db708af02&quot;, &quot;correlation_id&quot;: &quot;c43036d2-7c77-4405-9604-bd77e7273e7a&quot;, &quot;delivery_mode&quot;: 2, &quot;delivery_info&quot;: &#123; &quot;priority&quot;: 0, &quot;exchange&quot;: &quot;celery&quot;, &quot;routing_key&quot;: &quot;celery&quot; &#125;, &quot;body_encoding&quot;: &quot;base64&quot;, &quot;delivery_tag&quot;: &quot;e7e288b5-ecbb-4ec6-912c-f42eb92dbd72&quot; &#125;&#125; 2.5 Celery配置1234567CELERY_DEFAULT_QUEUE：默认队列BROKER_URL : 代理人的网址CELERY_RESULT_BACKEND：结果存储地址CELERY_TASK_SERIALIZER：任务序列化方式CELERY_RESULT_SERIALIZER：任务执行结果序列化方式CELERY_TASK_RESULT_EXPIRES：任务过期时间CELERY_ACCEPT_CONTENT：指定任务接受的内容序列化类型(序列化)，一个列表； 2.6 获取执行任务执行结果的方法123456789r = func.delay(...)r.ready() # 查看任务状态，返回布尔值, 任务执行完成, 返回 True, 否则返回 False.r.wait() # 等待任务完成, 返回任务执行结果，很少使用；r.get(timeout=1) # 获取任务执行结果，可以设置等待时间r.result # 任务执行结果.r.state # PENDING, START, SUCCESS，任务当前的状态r.status # PENDING, START, SUCCESS，任务当前的状态r.successful # 任务成功返回truer.traceback # 如果任务抛出了一个异常，你也可以获取原始的回溯信息 2.7 celery的装饰方法celery.task task()把任务（函数）装饰成异步 12345@celery.task()def func(): # do something pass 可以重新定义任务的基类 1234567891011class MyTask(celery.Task): # 任务失败时执行 def on_failure(self, exc, task_id, args, kwargs, einfo): print('&#123;0!r&#125; failed: &#123;1!r&#125;'.format(task_id, exc)) # 任务成功时执行 def on_success(self, retval, task_id, args, kwargs): pass # 任务重试时执行 def on_retry(self, exc, task_id, args, kwargs, einfo): pass 参数 task_id : 任务id einfo：执行失败时任务详情 exc： 失败时的错误类型 retval： 任务成功时返回的执行结果 2.8 一份完整的配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445# 注意，celery4版本后，CELERY_BROKER_URL改为BROKER_URLBROKER_URL = 'amqp://username:passwd@host:port/虚拟主机名'# 指定结果的接受地址CELERY_RESULT_BACKEND = 'redis://username:passwd@host:port/db'# 指定任务序列化方式CELERY_TASK_SERIALIZER = 'msgpack' # 指定结果序列化方式CELERY_RESULT_SERIALIZER = 'msgpack'# 任务过期时间,celery任务执行结果的超时时间CELERY_TASK_RESULT_EXPIRES = 60 * 20 # 指定任务接受的序列化类型.CELERY_ACCEPT_CONTENT = [\"msgpack\"] # 任务发送完成是否需要确认，这一项对性能有一点影响 CELERY_ACKS_LATE = True # 压缩方案选择，可以是zlib, bzip2，默认是发送没有压缩的数据CELERY_MESSAGE_COMPRESSION = 'zlib' # 规定完成任务的时间CELERYD_TASK_TIME_LIMIT = 5 # 在5s内完成任务，否则执行该任务的worker将被杀死，任务移交给父进程# celery worker的并发数，默认是服务器的内核数目,也是命令行-c参数指定的数目CELERYD_CONCURRENCY = 4 # celery worker 每次去rabbitmq预取任务的数量CELERYD_PREFETCH_MULTIPLIER = 4 # 每个worker执行了多少任务就会死掉，默认是无限的CELERYD_MAX_TASKS_PER_CHILD = 40 # 设置默认的队列名称，如果一个消息不符合其他的队列就会放在默认队列里面，如果什么都不设置的话，数据都会发送到默认的队列中CELERY_DEFAULT_QUEUE = \"default\" # 设置详细的队列CELERY_QUEUES = &#123; \"default\": &#123; # 这是上面指定的默认队列 \"exchange\": \"default\", \"exchange_type\": \"direct\", \"routing_key\": \"default\" &#125;, \"topicqueue\": &#123; # 这是一个topic队列 凡是topictest开头的routing key都会被放到这个队列 \"routing_key\": \"topic.#\", \"exchange\": \"topic_exchange\", \"exchange_type\": \"topic\", &#125;, \"task_eeg\": &#123; # 设置扇形交换机 \"exchange\": \"tasks\", \"exchange_type\": \"fanout\", \"binding_key\": \"tasks\", &#125;, &#125; 2.8 Celery定时任务 指定定时任务并加入配置 重新启动worker 123456789101112131415# config.pyfrom datetime import timedeltafrom celery.schedules import crontab CELERYBEAT_SCHEDULE = &#123; 'ptask': &#123; 'task': 'tasks.period_task', 'schedule': timedelta(seconds=5), &#125;,&#125;# 添加定时任务@app.task(bind=True)def period_task(self): print 'period task done: &#123;0&#125;'.format(self.request.id) PS:时间如果涉及到datatime最好设置为UTC时间 启动定时任务进程 1celery -A task beat 2.9 链式任务链式任务就是异步或者定时执行的任务由多个子任务执行完成 123456789101112131415161718def update_page_info(url): # fetch_page -&gt; parse_page -&gt; store_page chain = fetch_page.s(url) | parse_page.s() | store_page_info.s(url) chain() @app.task()def fetch_page(url): return myhttplib.get(url) @app.task()def parse_page(page): return myparser.parse_document(page) @app.task(ignore_result=True)def store_page_info(info, url): PageInfo.objects.create(url=url, info=info) fetch_page.apply_async((url), link=[parse_page.s(), store_page_info.s(url)]) 3 Celery, rabbitmq实现exchange三种模式 celery的工作流程 borker : 消息中间件 worker : 任务执行单元 storgae: 任务执行结果存储 3.1 exchange 常用模式介绍(direct, fanout, topic, header) 当我们执行的任务需要根据特定的需要进行分类时，我们可以对任务创建多个队列进行， 每一个队列交换方式可以指定，需要注意的是：redis只能提供 direct exchange 方式， 也是默认指定的方式，所以我们把中间人换成了rabbitmq。 首先我们来了解一下交换模式有哪些？ Direct Exchange 模式 这种模式是rabbitmq（redis）自带的一种模式，所以我们在实际使用过程中只要指定routing_key就可以了，或者是指定队列名称即可。 ps: 如果我们指定的队列名称不在配置里面，那我们创建的这条消息任务会被自动废除，所以需要检查下配置里的队列是否正确，因为rabbitmq只具备存储队列的能力，不能存储消息信息。 Fanout Exchange 模式 fanout模式不用指定routing_key， 所有exchange_ type 是fanout的 Queue都会执行 所以队列和fanout可以多对多绑定。 Topic Exchange 模式 任何发送到Topic Exchange的消息都会被转发到所有关心RouteKey中指定话题的Queue上 这种模式较为复杂，简单来说，就是每个队列都有其关心的主题，所有的消息都带有一个“标题”(RouteKey)，Exchange会将消息转发到所有关注主题能与RouteKey模糊匹配的队列。 这种模式需要RouteKey，也许要提前绑定Exchange与Queue。 在进行绑定时，要提供一个该队列关心的主题，如“#.log.#”表示该队列关心所有涉及log的消息(一个RouteKey为”MQ.log.error”的消息会被转发到该队列)。 “#”表示0个或若干个关键字，“”表示一个关键字。如“log.”能与“log.warn”匹配，无法与“log.warn.timeout”匹配；但是“log.#”能与上述两者匹配。 同样，如果Exchange没有发现能够与RouteKey匹配的Queue，则会抛弃此消息。 3.2 在实际例子中如何去运用这三种模式 首先要安装rabitmq并且启动 rabbitmq-server 创建rabbitmq_config.py 文件， 并且把之前在tasks.py中引用的配置修改为rabbitmq_config，代码如下 1234567891011121314#coding:utf-8from celery.schedules import crontabimport sysimport ossys.path.insert(0, os.getcwd())CELERY_IMPORTS = (&quot;tasks&quot;, )CELERY_RESULT_BACKEND = &quot;amqp&quot;BROKER_HOST = &quot;localhost&quot;BROKER_PORT = 5672BROKER_USER = &quot;guest&quot;BROKER_PASSWORD = &quot;guest&quot;BROKER_VHOST = &quot;/&quot; 创建需要的交换方式 123456789101112131415161718default_exchange = Exchange('dedfault', type='direct')# 定义一个媒体交换机,类型是直连交换机media_exchange = Exchange('media', type='direct')# 定义一个image交换机,类型是fanout交换机image_exchange = Exchange('media', type='direct')# 创建三个队列，一个是默认队列，一个是video、一个imageCELERY_QUEUES = ( Queue('default', default_exchange, routing_key='default'), Queue('videos', media_exchange, routing_key='media.video'), Queue('images', media_exchange, routing_key='media.image'))# 定义默认队列和默认的交换机routing_keyCELERY_DEFAULT_QUEUE = 'default'CELERY_DEFAULT_EXCHANGE = 'default'CELERY_DEFAULT_ROUTING_KEY = 'default' 在tasks.py中指定任务 123456789101112131415161718192021222324252627282930# 视频压缩@app.taskdef video_compress(video_name): time.sleep(10) print('Compressing the:', video_name) return 'success'@app.taskdef video_upload(video_name): time.sleep(5) print( u'正在上传视频') return 'success'# 压缩照片@app.taskdef image_compress(image_name): time.sleep(10) print('Compressing the:', image_name) return 'success'# 其他任务@app.taskdef other(str): time.sleep(10) print ('Do other things') return 'success' 指定路由 1234567891011CELERY_ROUTES = (&#123;'tasks.image_compress': &#123; 'queue': 'images', 'routing_key': 'media.image' &#125;&#125;,&#123;'tasks.video_upload': &#123; 'queue': 'videos', 'routing_key': 'media.video' &#125;&#125;,&#123;'tasks.video_compress': &#123; 'queue': 'videos', 'routing_key': 'media.video' &#125;&#125;, ) 现在执行创建的任务在启动worker的时候可以分两种启动方式 第一种： 指定Queue 第二种 ： 不指定（全部执行） ps 为了更好的看到我们添加的队列，还有相应的交换模式，启动全部的队列 启动worker [queue]中包含了创建的队列，其他参数本文前面可以对照[tasks]中显示了我们所有的任务 123456789101112131415161718192021222324252627---- **** ----- --- * *** * -- Darwin-18.2.0-x86_64-i386-64bit 2018-12-28 15:38:00-- * - **** --- - ** ---------- [config]- ** ---------- .&gt; app: tasks:0x104e78d68- ** ---------- .&gt; transport: amqp://guest:**@localhost:5672//- ** ---------- .&gt; results: amqp://- *** --- * --- .&gt; concurrency: 4 (prefork)-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)--- ***** ----- -------------- [queues] .&gt; default exchange=dedfault(direct) key=default .&gt; images exchange=media(direct) key=media.image .&gt; others exchange=other(fanout) key=other.others .&gt; videos exchange=media(direct) key=media.video[tasks] . tasks.add . tasks.dr . tasks.image_compress . tasks.other . tasks.period_task . tasks.task . tasks.video_compress . tasks.video_upload[2018-12-28 15:38:00,906: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672// 执行结果(可以在rabbimq后台管理中看相关执行结果)： 12345678910111213[2018-12-28 15:38:00,906: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//[2018-12-28 15:38:00,933: INFO/MainProcess] mingle: searching for neighbors[2018-12-28 15:38:02,013: INFO/MainProcess] mingle: all alone[2018-12-28 15:38:02,091: INFO/MainProcess] celery@zhanlingjiedeMacBook-Pro.local ready.[2018-12-28 15:38:42,386: INFO/MainProcess] Received task: tasks.add[1fdfbc23-e106-49ab-ac25-d46c2b5e8960] [2018-12-28 15:38:42,429: INFO/ForkPoolWorker-3] Task tasks.add[1fdfbc23-e106-49ab-ac25-d46c2b5e8960] succeeded in 0.040455893002217636s: 5[2018-12-28 15:38:46,397: INFO/MainProcess] Received task: tasks.image_compress[cab797c5-eaae-4f11-b55c-041f4256ead9] [2018-12-28 15:38:46,410: INFO/MainProcess] Received task: tasks.other[0b00fd52-2251-42ef-9743-49df3f2906ed] [2018-12-28 15:38:56,401: WARNING/ForkPoolWorker-4] Compressing the:[2018-12-28 15:38:56,402: WARNING/ForkPoolWorker-4] 这是我上传的图片[2018-12-28 15:38:56,412: WARNING/ForkPoolWorker-3] Do other things[2018-12-28 15:38:56,447: INFO/ForkPoolWorker-3] Task tasks.other[0b00fd52-2251-42ef-9743-49df3f2906ed] succeeded in 10.036200570997607s: 'success'[2018-12-28 15:38:56,461: INFO/ForkPoolWorker-4] Task tasks.image_compress[cab797c5-eaae-4f11-b55c-041f4256ead9] succeeded in 10.061314186998061s: 'success' 补充一般在使用celery为了和实际场景结合会使用框架去使用 django + celery + redis(rabbitmq)","categories":[{"name":"python","slug":"python","permalink":"https://blog.jayzhan.cn/categories/python/"}],"tags":[{"name":"django-celery","slug":"django-celery","permalink":"https://blog.jayzhan.cn/tags/django-celery/"}]},{"title":"Continue writing","slug":"Continue-writing","date":"2019-02-03T18:57:16.000Z","updated":"2019-02-03T19:02:47.375Z","comments":true,"path":"2019/02/04/Continue-writing/","link":"","permalink":"https://blog.jayzhan.cn/2019/02/04/Continue-writing/","excerpt":"","text":"从2017-8开始写自己的博客，主要是想对自己学习的东西做一个记录，为了方便后期解决问题查阅 经过两年的多的时间已经养成了习惯，与其说是博客用笔记更为恰当 很可惜在前段时间电脑重装了下，仓库也没有备份，写一篇继续篇再次记录自己的学习路程 开始于2019-2-4 大年三十","categories":[{"name":"随记","slug":"随记","permalink":"https://blog.jayzhan.cn/categories/随记/"}],"tags":[{"name":"feelings","slug":"feelings","permalink":"https://blog.jayzhan.cn/tags/feelings/"}]}]}